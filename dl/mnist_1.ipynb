{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 5214 and ELEC 5680\n",
    "## Assignment 1 | Yin, Tianci (20587470)\n",
    "---\n",
    "In this problem, we will implement a model to classify digits using the MNIST\n",
    "dataset. The dataset is available at https://git-disl.github.io/GTDLBench/\n",
    "datasets/mnist_datasets/ or https://pytorch.org/vision/stable/generated/\n",
    "torchvision.datasets.MNIST.html. Please use both training and test sets.\n",
    "You can train your model for 20 epochs. Use a batch size of 64 to train the\n",
    "model. Please submit a report and code for this assignment. You can use\n",
    "Pytorch or Tensorflow in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined hyperparameters\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours (25 Points)\n",
    "---\n",
    "We will implement a K-nearest neighbor model that finds the most similar image\n",
    "given a test image. We can use the scikit-learn Python library to find the KNN.\n",
    "We will use the sum of absolute difference (SAD) on all the pixels to measure\n",
    "the similarity between two images. What will be the accuracy if we find the\n",
    "nearest neighbor? What will be the accuracy if we find K neighbors? When\n",
    "we find K, we choose the label that appears most among the KNN images (if\n",
    "there is a tie, we pick the one with the smallest aggregated SAD). Plot a curve\n",
    "of accuracy versus K from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# predefined\n",
    "K_CANDIDATES = [i for i in range(1, 11)]  # number of neighbors\n",
    "\n",
    "def load_mnist_flat(proportion=1.0):\n",
    "    # setup\n",
    "    train_set = datasets.MNIST(\"./_data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "    test_set = datasets.MNIST(\"./_data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # load into numpy arrays with flattened features\n",
    "    train_X = train_set.data.numpy().reshape(len(train_set), -1)\n",
    "    train_y = train_set.targets.numpy()\n",
    "    test_X = test_set.data.numpy().reshape(len(test_set), -1)\n",
    "    test_y = test_set.targets.numpy()\n",
    "    \n",
    "    # proportion\n",
    "    if proportion < 1.0:\n",
    "        train_X = train_X[:int(proportion*len(train_X))]\n",
    "        train_y = train_y[:int(proportion*len(train_y))]\n",
    "        test_X = test_X[:int(proportion*len(test_X))]\n",
    "        test_y = test_y[:int(proportion*len(test_y))]\n",
    "\n",
    "    print(\"MNIST\\ntrain shapes:\", train_X.shape, train_y.shape, \"\\ntest shapes:\", test_X.shape, test_y.shape)\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After digging through `sklearn.neighbors.KNeighborsClassifier` a little, it seems like the default tie-break behavior is based on training data ordering: \n",
    "\n",
    "> From [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier:~:text=Regarding%20the%20Nearest%20Neighbors%20algorithms%2C%20if%20it%20is%20found%20that%20two%20neighbors%2C%20neighbor%20k%2B1%20and%20k%2C%20have%20identical%20distances%20but%20different%20labels%2C%20the%20results%20will%20depend%20on%20the%20ordering%20of%20the%20training%20data.): \n",
    "> _\"Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1 and k, have identical distances but different labels, the results will depend on the ordering of the training data.\"_\n",
    "\n",
    "And there doesn't appear to be a way to customize that behavior ([Github Issue](https://github.com/scikit-learn/scikit-learn/issues/21006)).\n",
    "\n",
    "Though according to:\n",
    "\n",
    "> From [StackExchange](https://stats.stackexchange.com/questions/144718/how-does-scikit-learn-resolve-ties-in-the-knn-classification): \n",
    "> _\"Digging a little deeper, the used neigh_ind array is the result of calling the kneighbors method, which (though the documentation doesn't say so) appears to return results in sorted order. So ties should be broken by choosing the class with the point closest to the query point, but this behavior isn't documented and I'm not 100% sure it always happens.\"_ @Danica\n",
    "\n",
    "> From [StackExchange](https://stats.stackexchange.com/questions/144718/how-does-scikit-learn-resolve-ties-in-the-knn-classification): \n",
    "> _\"Seems like the sorting only happens for the brute-force approach, and not the tree-based method?\"_ @AllanLRH\n",
    "\n",
    "we could make use of this behavior, but this still only returns the neighbor with the smallest **individual** SAD, not the smallest **aggregated** SAD.\n",
    "\n",
    "We give up and implement the classification as well as the tie-break behavior on our own..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier_L1(k, train_X, train_y, test_X):\n",
    "    # sum of absolute difference (L1 distance / manhattan distance)\n",
    "    model = NearestNeighbors(n_neighbors=k, metric=\"manhattan\", n_jobs=-1)\n",
    "    model.fit(train_X)\n",
    "\n",
    "    # get nearest neighbors\n",
    "    dist, idx = model.kneighbors(test_X, return_distance=True)\n",
    "    \n",
    "    # predict test set\n",
    "    pred = []\n",
    "\n",
    "    # custom predict logic\n",
    "    for i in range(len(test_X)):\n",
    "        # for each test image, get the most common labels of the nearest neighbors\n",
    "        neighbor_y = train_y[idx[i]]\n",
    "        unique_y, count_y = np.unique(neighbor_y, return_counts=True)\n",
    "        common_y = unique_y[count_y == np.max(count_y)] # shapes: (10000, k)\n",
    "        \n",
    "        # tie-break behavior\n",
    "        if len(common_y) != 1:\n",
    "            aggregated_sad = {}\n",
    "            for y in common_y:\n",
    "                common_y_idx = np.where(neighbor_y == y)[0] # get idx of common_y\n",
    "                aggregated_sad[y] = np.sum(dist[i][common_y_idx]) # get sum of SAD\n",
    "            \n",
    "            pred.append(min(aggregated_sad, key=aggregated_sad.get))\n",
    "        else:\n",
    "            pred.append(common_y[0])\n",
    "    \n",
    "    return np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the accuracy of our classifier using Ks ranging from 1 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "train shapes: (60000, 784) (60000,) \n",
      "test shapes: (10000, 784) (10000,)\n",
      "K=1, accuracy=0.96310000\n",
      "K=2, accuracy=0.96310000\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "# load mnist data\n",
    "train_X, train_y, test_X, test_y = load_mnist_flat(proportion=1.0)\n",
    "\n",
    "# iterate over k candidates\n",
    "for k in K_CANDIDATES:\n",
    "    # fit and predict our custom knn classifier\n",
    "    pred = knn_classifier_L1(k, train_X, train_y, test_X)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    acc = np.mean(pred == test_y) # the mean of pred T/F array\n",
    "    accuracies.append(acc)\n",
    "    print(f\"K={k}, accuracy={acc:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K_CANDIDATES, accuracies, marker=\"o\")\n",
    "plt.title(\"KNN L1 Classifier: Accuracy vs K\")\n",
    "plt.xlabel(\"K Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.xticks(K_CANDIDATES)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
